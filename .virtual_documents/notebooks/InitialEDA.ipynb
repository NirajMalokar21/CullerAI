import pandas as pd
import cv2
import os


mos_df = pd.read_csv("../data/MOS.csv") 
print(mos_df.head())





img_path = os.path.join("../data/distorted_images", mos_df['image_id'][0])
img = cv2.imread(img_path)
print(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)





import numpy as np
def extract_features(img):
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    # Sharpness
    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()

    # Brightness & noise
    brightness = np.mean(gray)
    noise = np.std(gray)

    # Color channel stats
    mean_b, mean_g, mean_r = np.mean(img, axis=(0,1))
    std_b, std_g, std_r = np.std(img, axis=(0,1))

    # Entropy
    hist, _ = np.histogram(gray, bins=256, range=(0,256), density=True)
    entropy_val = -np.sum(hist * np.log2(hist + 1e-7))

    # Edge density
    edges = cv2.Canny(gray, 100, 200)
    edge_density = np.sum(edges > 0) / edges.size

    return [
        sharpness, brightness, noise,
        mean_b, mean_g, mean_r,
        std_b, std_g, std_r,
        entropy_val, edge_density
    ]





features = []
labels = []

for idx, row in mos_df.iterrows():
    img_path = os.path.join("../data/distorted_images", row['image_id'])
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    features.append(extract_features(img))
    labels.append(row['mean'])

import pandas as pd

columns = [
    'sharpness', 'brightness', 'noise',
    'mean_b', 'mean_g', 'mean_r',
    'std_b', 'std_g', 'std_r',
    'entropy', 'edge_density'
]

features_df = pd.DataFrame(features, columns=columns)
features_df['mean'] = labels


features_df.describe()


from sklearn.ensemble import GradientBoostingRegressor

gbm = GradientBoostingRegressor(random_state=42) 

gbm_param_dist = {
    'n_estimators': [400, 600, 800, 1000],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5, 6],
    'min_samples_split': [5, 10, 15],
    'min_samples_leaf': [2, 4, 6],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'max_features': ['sqrt', 'log2', None]
}


from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=gbm,
    param_distributions=gbm_param_dist,
    n_iter=50,                # 50 random combinations
    cv=5,                     # 5-fold cross-validation
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=2,
    random_state=42
)


gbm_params = {
    "loss": "squared_error",
    "learning_rate": 0.05,
    "n_estimators": 800,
    "subsample": 0.7, 
    "criterion": "friedman_mse",
    "min_samples_split": 10,
    "min_samples_leaf": 5,
    "min_weight_fraction_leaf": 0.0,
    "max_depth": 5,
    "min_impurity_decrease": 0.0,
    "init": None,
    "random_state": 42,
    "max_features": "log2",
    "alpha": 0.9,
    "verbose": 0,
    "max_leaf_nodes": None,
    "warm_start": False,
    "validation_fraction": 0.1,
    "n_iter_no_change": None,
    "tol": 0.0001,
    "ccp_alpha": 0.0
}



from sklearn.model_selection import train_test_split

X = features_df.drop(columns=['mean'])
y = features_df['mean']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


best_gbm = GradientBoostingRegressor(**gbm_params) 

best_gbm.fit(X_train, y_train)


y_pred = best_gbm.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error
print("Test MSE:", mean_squared_error(y_test, y_pred))
print("Test MAE:", mean_absolute_error(y_test, y_pred)) 


print("Train MSE:", mean_squared_error(y_train, best_gbm.predict(X_train)))
print("Test MSE:", mean_squared_error(y_test, best_gbm.predict(X_test))) 
